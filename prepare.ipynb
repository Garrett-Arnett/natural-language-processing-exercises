{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be590f28-c292-42c1-892f-e8e7434a2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire as ac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70deca1-de78-47c6-b1a8-bf5ac4951307",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "## 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a8a2c7-0d2c-422f-8d47-5d5257f5ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    '''\n",
    "    This function takes in a article in string format.\n",
    "    \n",
    "    Turns all letters into lowercase.\n",
    "    \n",
    "    Normalizes the unicode characters using the NFKD method,\n",
    "    while ignoring any unknow characters.\n",
    "    \n",
    "    Will replace anything that is NOT letters, numbers, whitespace or single quote.\n",
    "    \n",
    "    This funtion will return a basic cleaned article in string format\n",
    "    '''\n",
    "    \n",
    "    # Lowercase \n",
    "    article = article.lower()\n",
    "    \n",
    "    # Normalization\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Replace\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821c3976-fb1f-4c3f-a0d4-e7c800f7341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to\\\n",
    "the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often\\\n",
    "incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb08dc55-b33b-4ef6-8295-8a7afdfb817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = basic_clean(article)\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370c11e-4daf-4ed7-9064-62d2e6508b6c",
   "metadata": {},
   "source": [
    "## 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f607343-57c0-4d5e-ac1c-1353992237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Creates a tokenizer using nltk.\n",
    "    Uses the tokenize on the artical and returns the article in string fromat.\n",
    "    '''\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    tokenizer.tokenize(article, return_str = True)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2dd884-4238-4a0d-8c89-b38c51148f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = tokenize(article)\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd4aaf-93a9-4ba7-a907-c2199383a9bd",
   "metadata": {},
   "source": [
    "## 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1889ec29-89f6-4b71-8e2a-5d84668e97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Creates a porter stemmer.\n",
    "    Applies the stemmer to each word in the article/string.\n",
    "    Joins the stems into a single string called article_stemmed.\n",
    "    Returns article_stemmed with all stemed characters. \n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    \n",
    "    # Join stems\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    \n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f92c58-492a-433c-820c-e1655b66f0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya are influenti hungarian mathematician who contribut a lot toth field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is oftenincorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_stemmed = stem(article)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33991212-a0e5-4e2c-b90f-eba053a21a5c",
   "metadata": {},
   "source": [
    "## 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb19902-17c8-4e8a-beda-210d8e7b691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Downloads 'wordnet' from nltk.\n",
    "    Creates a lemmatizer.\n",
    "    Applies the lemmatizer to each word in the article/string.\n",
    "    Joins the lemmmas into a single string called article_lemmatized.\n",
    "    Returns artical_lemmatized with all lemmatized characters. \n",
    "    '''\n",
    "    # Download the first time.\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    # Create the Lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Apply the lemmatize to each word in our string.\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    \n",
    "    # Join lemmas\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec58e3c-8f10-4335-af14-6c52fa917f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematician who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written a erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_lemmatized = lemmatize(article)\n",
    "article_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789c03a-37cc-4747-80bb-df7884fd9ca1",
   "metadata": {},
   "source": [
    "## 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b13db9f-21a1-4566-afa7-56c8d57de2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b925990-463e-4e4b-a927-f77fc1c40659",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = 'my'\n",
    "exclude_words = 'are'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "261b64a2-4a5e-4447-aa02-ff282a4ac75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematicians contributed lot tothe field erdos's name contains hungarian letter 'o' 'o' double acute accent oftenincorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb2496-b173-4cf0-bb65-168b16addbef",
   "metadata": {},
   "source": [
    "## 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdf26a0e-1cac-4c6d-9ad5-a11ccdb5aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46649e3a-15f5-4c3d-af9e-0c940cc87943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article, category):\n",
    "    \"\"\"\n",
    "    This function takes in a category and artical as a string. \n",
    "    Category must be an available category in inshorts, article is the link.\n",
    "    Returns a single inshort article.\n",
    "    \"\"\"\n",
    "    # Attribute selector\n",
    "    title = article.select(\"[itemprop='headline']\")[0].text\n",
    "    \n",
    "    # article body\n",
    "    content = article.select(\"[itemprop='articleBody']\")[0].text\n",
    "    \n",
    "    output = {}\n",
    "    output[\"title\"] = title\n",
    "    output[\"content\"] = content\n",
    "    output[\"category\"] = category\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52ac62cc-6fcb-4673-90c8-9a69c083e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test my function on the business page\n",
    "business_test = ac.scrape_one_page('business')\n",
    "topics = ['business', 'sports', 'technology', 'entertainment']\n",
    "\n",
    "news_df = ac.get_news_articles(topics)\n",
    "\n",
    "news_df = pd.DataFrame(ac.get_news_articles(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "631885ab-875f-4716-a38c-a994e9652405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ratan Tata must check whether his donations re...</td>\n",
       "      <td>Wrestler Vinesh Phogat on Wednesday said, \"I r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>I was laid off from LinkedIn before even start...</td>\n",
       "      <td>Lea Schuhmacher, a woman who signed the contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>India's high streets for shopping ranked, Beng...</td>\n",
       "      <td>Bengaluru's MG Road ranked first in the list o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>PepsiCo bottler Varun Beverages enters ₹1 lakh...</td>\n",
       "      <td>Varun Beverages, PepsiCo's India franchise bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>All eyes on India as it becomes a 'plus 1' to ...</td>\n",
       "      <td>Vedanta Resources Chairman Anil Agarwal said a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Papa used to tell friends that I'll be a polic...</td>\n",
       "      <td>Actress Sonakshi Sinha revealed that when she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Your so called whatevers not getting opening: ...</td>\n",
       "      <td>Actress Richa Chadha took to Instagram and urg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>'Kantara' actor Rishab Shetty casts vote in Ka...</td>\n",
       "      <td>Actor-filmmaker Rishab Shetty on Wednesday arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Didn’t tell, thought let work go as it is: Bin...</td>\n",
       "      <td>Veteran actress Bindu, who appeared in several...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>UP CM meets makers of 'The Kerala Story' in Lu...</td>\n",
       "      <td>UP CM Yogi Adityanath on Wednesday met the mak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                              title   \n",
       "0        business  Ratan Tata must check whether his donations re...  \\\n",
       "1        business  I was laid off from LinkedIn before even start...   \n",
       "2        business  India's high streets for shopping ranked, Beng...   \n",
       "3        business  PepsiCo bottler Varun Beverages enters ₹1 lakh...   \n",
       "4        business  All eyes on India as it becomes a 'plus 1' to ...   \n",
       "..            ...                                                ...   \n",
       "95  entertainment  Papa used to tell friends that I'll be a polic...   \n",
       "96  entertainment  Your so called whatevers not getting opening: ...   \n",
       "97  entertainment  'Kantara' actor Rishab Shetty casts vote in Ka...   \n",
       "98  entertainment  Didn’t tell, thought let work go as it is: Bin...   \n",
       "99  entertainment  UP CM meets makers of 'The Kerala Story' in Lu...   \n",
       "\n",
       "                                              content  \n",
       "0   Wrestler Vinesh Phogat on Wednesday said, \"I r...  \n",
       "1   Lea Schuhmacher, a woman who signed the contra...  \n",
       "2   Bengaluru's MG Road ranked first in the list o...  \n",
       "3   Varun Beverages, PepsiCo's India franchise bot...  \n",
       "4   Vedanta Resources Chairman Anil Agarwal said a...  \n",
       "..                                                ...  \n",
       "95  Actress Sonakshi Sinha revealed that when she ...  \n",
       "96  Actress Richa Chadha took to Instagram and urg...  \n",
       "97  Actor-filmmaker Rishab Shetty on Wednesday arr...  \n",
       "98  Veteran actress Bindu, who appeared in several...  \n",
       "99  UP CM Yogi Adityanath on Wednesday met the mak...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7db92b-59ae-48c1-84dc-191a9178f00e",
   "metadata": {},
   "source": [
    "## 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3788ddcf-ea7d-445a-b86c-4e5ab3f674e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = pd.DataFrame(ac.get_blog_articles('blog_posts.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f4a9c27-8493-438f-821a-60e51e421d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date_published</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Mar 28, 2023</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Magdalen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Mar 20, 2023</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Rachel R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mellor</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Mar 13, 2023</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Sarah Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Mar 6, 2023</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Madelein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black Excellence in Tech: Panelist Spotlight –...</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Feb 16, 2023</td>\n",
       "      <td>\\nBlack excellence in tech: Panelist Spotlight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black excellence in tech: Panelist Spotlight –...</td>\n",
       "      <td>https://codeup.com/events/black-excellence-in-...</td>\n",
       "      <td>Feb 13, 2023</td>\n",
       "      <td>\\nBlack excellence in tech: Panelist Spotlight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0  Women in tech: Panelist Spotlight – Magdalena ...  \\\n",
       "1  Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "2   Women in Tech: Panelist Spotlight – Sarah Mellor   \n",
       "3  Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "4  Black Excellence in Tech: Panelist Spotlight –...   \n",
       "5  Black excellence in tech: Panelist Spotlight –...   \n",
       "\n",
       "                                                link date_published   \n",
       "0  https://codeup.com/events/black-excellence-in-...   Mar 28, 2023  \\\n",
       "1  https://codeup.com/events/black-excellence-in-...   Mar 20, 2023   \n",
       "2  https://codeup.com/events/black-excellence-in-...   Mar 13, 2023   \n",
       "3  https://codeup.com/events/black-excellence-in-...    Mar 6, 2023   \n",
       "4  https://codeup.com/events/black-excellence-in-...   Feb 16, 2023   \n",
       "5  https://codeup.com/events/black-excellence-in-...   Feb 13, 2023   \n",
       "\n",
       "                                             content  \n",
       "0  \\nWomen in tech: Panelist Spotlight – Magdalen...  \n",
       "1  \\nWomen in tech: Panelist Spotlight – Rachel R...  \n",
       "2  \\nWomen in tech: Panelist Spotlight – Sarah Me...  \n",
       "3  \\nWomen in tech: Panelist Spotlight – Madelein...  \n",
       "4  \\nBlack excellence in tech: Panelist Spotlight...  \n",
       "5  \\nBlack excellence in tech: Panelist Spotlight...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d13e2f-3729-4e28-af29-87c6f9720b18",
   "metadata": {},
   "source": [
    "## 8. For each dataframe, produce the following columns:\n",
    "\n",
    "* title to hold the title\n",
    "* original to hold the original article/post content\n",
    "* clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "* stemmed to hold the stemmed version of the cleaned data.\n",
    "* lemmatized to hold the lemmatized version of the cleaned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ee7b248-60cf-4e94-ae20-2cba94da0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    woman tech panelist spotlight magdalena rahn c...\n",
       "1    woman tech panelist spotlight rachel robbinsma...\n",
       "2    woman tech panelist spotlight sarah mellor cod...\n",
       "3    woman tech panelist spotlight madeleine capper...\n",
       "4    black excellence tech panelist spotlight wilma...\n",
       "5    black excellence tech panelist spotlight steph...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df['content'].apply(basic_clean).apply(tokenize).apply(lemmatize).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29648efe-b19d-47c9-8939-b83c39f2f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    #original text from content column\n",
    "    df['original'] = df['content']\n",
    "    \n",
    "    #chain together clean, tokenize, remove stopwords\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    #chain clean, tokenize, stem, remove stopwords\n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    #clean clean, tokenize, lemmatize, remove stopwords\n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', 'original', 'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea353fdb-059b-45a2-a638-951b9d0ef4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Magdalen...</td>\n",
       "      <td>women tech panelist spotlight magdalena rahn c...</td>\n",
       "      <td>women tech panelist spotlight magdalena rahn c...</td>\n",
       "      <td>woman tech panelist spotlight magdalena rahn c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Rachel R...</td>\n",
       "      <td>women tech panelist spotlight rachel robbinsma...</td>\n",
       "      <td>women tech panelist spotlight rachel robbinsma...</td>\n",
       "      <td>woman tech panelist spotlight rachel robbinsma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mellor</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Sarah Me...</td>\n",
       "      <td>women tech panelist spotlight sarah mellor cod...</td>\n",
       "      <td>women tech panelist spotlight sarah mellor cod...</td>\n",
       "      <td>woman tech panelist spotlight sarah mellor cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>\\nWomen in tech: Panelist Spotlight – Madelein...</td>\n",
       "      <td>women tech panelist spotlight madeleine capper...</td>\n",
       "      <td>women tech panelist spotlight madelein capper ...</td>\n",
       "      <td>woman tech panelist spotlight madeleine capper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black Excellence in Tech: Panelist Spotlight –...</td>\n",
       "      <td>\\nBlack excellence in tech: Panelist Spotlight...</td>\n",
       "      <td>black excellence tech panelist spotlight wilma...</td>\n",
       "      <td>black excel tech panelist spotlight wilmari de...</td>\n",
       "      <td>black excellence tech panelist spotlight wilma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0  Women in tech: Panelist Spotlight – Magdalena ...  \\\n",
       "1  Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "2   Women in Tech: Panelist Spotlight – Sarah Mellor   \n",
       "3  Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "4  Black Excellence in Tech: Panelist Spotlight –...   \n",
       "\n",
       "                                            original   \n",
       "0  \\nWomen in tech: Panelist Spotlight – Magdalen...  \\\n",
       "1  \\nWomen in tech: Panelist Spotlight – Rachel R...   \n",
       "2  \\nWomen in tech: Panelist Spotlight – Sarah Me...   \n",
       "3  \\nWomen in tech: Panelist Spotlight – Madelein...   \n",
       "4  \\nBlack excellence in tech: Panelist Spotlight...   \n",
       "\n",
       "                                               clean   \n",
       "0  women tech panelist spotlight magdalena rahn c...  \\\n",
       "1  women tech panelist spotlight rachel robbinsma...   \n",
       "2  women tech panelist spotlight sarah mellor cod...   \n",
       "3  women tech panelist spotlight madeleine capper...   \n",
       "4  black excellence tech panelist spotlight wilma...   \n",
       "\n",
       "                                             stemmed   \n",
       "0  women tech panelist spotlight magdalena rahn c...  \\\n",
       "1  women tech panelist spotlight rachel robbinsma...   \n",
       "2  women tech panelist spotlight sarah mellor cod...   \n",
       "3  women tech panelist spotlight madelein capper ...   \n",
       "4  black excel tech panelist spotlight wilmari de...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  woman tech panelist spotlight magdalena rahn c...  \n",
       "1  woman tech panelist spotlight rachel robbinsma...  \n",
       "2  woman tech panelist spotlight sarah mellor cod...  \n",
       "3  woman tech panelist spotlight madeleine capper...  \n",
       "4  black excellence tech panelist spotlight wilma...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_codeup = prep_article_data(codeup_df, 'content', extra_words =[], exclude_words=[])\n",
    "\n",
    "#take a look\n",
    "prep_codeup.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6708cce7-ac71-44d8-88b7-662cfa4bd099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         Women in tech: Panelist Spotlight – Rachel Rob...\n",
       "original      \\nWomen in tech: Panelist Spotlight – Rachel R...\n",
       "clean         women tech panelist spotlight rachel robbinsma...\n",
       "stemmed       women tech panelist spotlight rachel robbinsma...\n",
       "lemmatized    woman tech panelist spotlight rachel robbinsma...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_codeup.iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d0216fa-8c55-4657-9c2e-e6a19368b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrettarnett/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ratan Tata must check whether his donations re...</td>\n",
       "      <td>Wrestler Vinesh Phogat on Wednesday said, \"I r...</td>\n",
       "      <td>wrestler vinesh phogat wednesday said request ...</td>\n",
       "      <td>wrestler vinesh phogat wednesday said request ...</td>\n",
       "      <td>wrestler vinesh phogat wednesday said request ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was laid off from LinkedIn before even start...</td>\n",
       "      <td>Lea Schuhmacher, a woman who signed the contra...</td>\n",
       "      <td>lea schuhmacher woman signed contract fulltime...</td>\n",
       "      <td>lea schuhmach woman sign contract fulltim job ...</td>\n",
       "      <td>lea schuhmacher woman signed contract fulltime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India's high streets for shopping ranked, Beng...</td>\n",
       "      <td>Bengaluru's MG Road ranked first in the list o...</td>\n",
       "      <td>bengaluru's mg road ranked first list india's ...</td>\n",
       "      <td>bengaluru' mg road rank first list india' top ...</td>\n",
       "      <td>bengaluru's mg road ranked first list india's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PepsiCo bottler Varun Beverages enters ₹1 lakh...</td>\n",
       "      <td>Varun Beverages, PepsiCo's India franchise bot...</td>\n",
       "      <td>varun beverages pepsico's india franchise bott...</td>\n",
       "      <td>varun beverag pepsico' india franchis bottler ...</td>\n",
       "      <td>varun beverage pepsico's india franchise bottl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All eyes on India as it becomes a 'plus 1' to ...</td>\n",
       "      <td>Vedanta Resources Chairman Anil Agarwal said a...</td>\n",
       "      <td>vedanta resources chairman anil agarwal said e...</td>\n",
       "      <td>vedanta resourc chairman anil agarw said eye i...</td>\n",
       "      <td>vedanta resource chairman anil agarwal said ey...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0  Ratan Tata must check whether his donations re...  \\\n",
       "1  I was laid off from LinkedIn before even start...   \n",
       "2  India's high streets for shopping ranked, Beng...   \n",
       "3  PepsiCo bottler Varun Beverages enters ₹1 lakh...   \n",
       "4  All eyes on India as it becomes a 'plus 1' to ...   \n",
       "\n",
       "                                            original   \n",
       "0  Wrestler Vinesh Phogat on Wednesday said, \"I r...  \\\n",
       "1  Lea Schuhmacher, a woman who signed the contra...   \n",
       "2  Bengaluru's MG Road ranked first in the list o...   \n",
       "3  Varun Beverages, PepsiCo's India franchise bot...   \n",
       "4  Vedanta Resources Chairman Anil Agarwal said a...   \n",
       "\n",
       "                                               clean   \n",
       "0  wrestler vinesh phogat wednesday said request ...  \\\n",
       "1  lea schuhmacher woman signed contract fulltime...   \n",
       "2  bengaluru's mg road ranked first list india's ...   \n",
       "3  varun beverages pepsico's india franchise bott...   \n",
       "4  vedanta resources chairman anil agarwal said e...   \n",
       "\n",
       "                                             stemmed   \n",
       "0  wrestler vinesh phogat wednesday said request ...  \\\n",
       "1  lea schuhmach woman sign contract fulltim job ...   \n",
       "2  bengaluru' mg road rank first list india' top ...   \n",
       "3  varun beverag pepsico' india franchis bottler ...   \n",
       "4  vedanta resourc chairman anil agarw said eye i...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  wrestler vinesh phogat wednesday said request ...  \n",
       "1  lea schuhmacher woman signed contract fulltime...  \n",
       "2  bengaluru's mg road ranked first list india's ...  \n",
       "3  varun beverage pepsico's india franchise bottl...  \n",
       "4  vedanta resource chairman anil agarwal said ey...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_news = prep_article_data(news_df, 'content', extra_words =[], exclude_words=[])\n",
    "\n",
    "#take a look\n",
    "prep_news.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17de5af8-8ee5-4806-a130-033150929cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         I was laid off from LinkedIn before even start...\n",
       "original      Lea Schuhmacher, a woman who signed the contra...\n",
       "clean         lea schuhmacher woman signed contract fulltime...\n",
       "stemmed       lea schuhmach woman sign contract fulltim job ...\n",
       "lemmatized    lea schuhmacher woman signed contract fulltime...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_news.iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a5a00b1-bf46-4baa-88dc-bbb5567ae208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         Black Excellence in Tech: Panelist Spotlight –...\n",
       "original      \\nBlack excellence in tech: Panelist Spotlight...\n",
       "clean         black excellence tech panelist spotlight wilma...\n",
       "stemmed       black excel tech panelist spotlight wilmari de...\n",
       "lemmatized    black excellence tech panelist spotlight wilma...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_codeup.iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e022a0-5e26-42a8-a60d-43aa3466cd71",
   "metadata": {},
   "source": [
    "## 9. Ask yourself:\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    * lemmatize is slower, so smaller is ok to take longer.\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    * stem because dataset is larger and stemming is faster\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "    * stemming because it is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcec8ce-11c8-496f-aad7-65f8d02b6add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08839e3f-4397-4c4e-95ba-f66edf0eb658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b18572-4a98-4a09-8138-c1b8582eaef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff98e5-0f92-4396-a5e8-c738f40ea27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab86687-bb09-4610-98b0-fd89993da33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6980439-843e-4090-8ac2-d011d1074c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d9f51-4de8-4f6f-a564-5c21307ca99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a4f0f-1967-442b-bb06-3b31ffcc8c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b659d-e90f-4325-84fc-e9eab7caba8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5c34f-0032-4cb3-a625-d6868d29b7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203d649-ecf8-47af-9e3e-786ba992bb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8621a-db98-4b3c-ae62-4bf22b6f1e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d108bab-7e7f-488f-94a2-65a2ace883f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee856dd-197e-4b1a-88aa-96f1ecac12b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb8900-9a26-4937-a6e5-cc83bf2433dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61438b3-2b6b-48d9-94d3-70662319c21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b28924-70e1-4494-8ef8-17e4fd376bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5945e-e03b-4ffd-821f-ad217d48230d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5d78d-d509-414c-b523-68b2a433d891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
